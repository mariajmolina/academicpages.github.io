---
title: 'First-time attendance to the NOAA Hazardous Weather Testbed Spring Forecasting Experiment'
date: 2021-05-29
permalink: /posts/2021/05/blog-post-5/
tags:
  - forecasting
  - thunderstorms
  - community
---

I had the wonderful opportunity to attend the NOAA Hazardous Weather Testbed (HWT) Spring Forecasting Experiment (SFE) this week. Our group got to try out several new experimental models for prediction of severe thunderstorms alongside operational meteorologists. 

Virtual attendance
======
According to the [NOAA HWT](https://hwt.nssl.noaa.gov/),
>The NOAA Hazardous Weather Testbed is a joint project of the National Weather Service and the National Severe Storms Laboratory. The HWT provides a conceptual framework and a physical space to foster collaboration between research and operations to test and evaluate emerging technologies and science for NWS operations. The HWT was borne from the “Spring Program” which, for the last decade, has been used to test and evaluate new forecast models, techniques, and products to support NWS Storm Prediction Center forecast operations.

![](/posts/AMS101_0-1.png)

What is the NOAA HWT SFE?
------
Python’s history dates back to the late-1980s, developed by Guido van Rossum as a hobby project that started over a winter holiday season. It was designed to be an interpreted programming language with an emphasis on code readability. Python was first released in 1991 as version 0.9, and it was named after the show Monty Python’s Flying Circus, not the snake. Python at that point had capabilities to handle classes with inheritance, exception handling and functions. In 1994, Python version 1.0 was released with functional programming tools, like lambda, filter, and reduce. During the year 2000, Python version 2.0 was released, and added new features such as list comprehensions and the cycle -- detecting garbage collection system -- which is a process by which Python periodically frees memory no longer in use. In 2008, Python version 3.0 was released, which was designed to correct some flaws in the language and this version also lost backward compatibility with earlier versions of Python. As of about a year ago, Python version 2 is no longer supported. 

![](/posts/AMS2021_1-1.png)

If Python has been around for about 30 years now, why does it seem to have become widely adopted in just the last few years? There are several key reasons for this. Because of its code readability and easy interpretability, Python is an accessible high level language and is used across a broad range of fields beyond the Earth sciences. As a result, it has gained a large user base of active and supportive contributors, including software engineers, data analysts, network engineers, scientists, and mathematicians. Having been around for 30 years also means that the language has reached a mature phase, with a large ecosystem of packages and many tutorials and examples freely available to help users accomplish their goals. Python has also been used on many large scale projects with success, including at companies like Google, which has led to organizations investing in the language and learning materials for their workforces. So while lots of programming languages have been around for a long time, these reasons can partly explain the growth of Python’s popularity.

![](/posts/AMS2021_2-1.png)

Machine learning presence at the 2021 NOAA HWT SFE
------
Machine learning has a long and rich history, that extends further back than Python’s history. The 1950s and 1980s saw excitement in the field, including the development of key terms and algorithms, such as the perceptron algorithm which was the precursor to modern neural networks, and the development of the backpropagation algorithm, allowing for multi-layered models and improved parameter optimization. These advances also had periods of disappointment and reduced funding, two time periods known as AI winters, due in part to skepticism of the field’s progress and limited technology. More recently, further development of deep convolutional neural networks occurred. The ImageNet database was worked on for several years in the 2000s, which was a groundbreaking dataset consisting of over 14 million high quality labeled images for over 22-thousand categories, ideal for training models in a supervised learning framework. The objective of this dataset was to benchmark model development and to overcome challenges with model overfitting. This started the ImageNet Large-Scale Visual Recognition Challenge, and in 2012, a large drop in error was recorded by the winning model, which was a convolutional neural network. Its high performance was attributed to its depth.

![](/posts/AMS2021_3-1.png)

Forecasting severe thunderstorms continues to be a challenge
------
In my postdoc research, I was able to leverage the power of Python and machine learning to conduct my work efficiently. The goal of my research was as follows: I had many thunderstorms extracted from a convection permitting climate simulation and then I trained a convolutional neural network to classify these storms into two categories. The machine learning workflow in a supervised learning framework can be generally summarized into three steps, step one being data preprocessing in preparation for model training, step two involving the training of the machine learning model, which was a convolutional neural network, and step three involving the evaluation of the model using unseen test data. During step three, machine learning interpretability can also be conducted to help explain the reasons for model skill, and I conducted this using saliency maps. 

![](/posts/AMS2021_4-1.png)

During my talk, I gave a high-level overview of how to navigate the various stages of a machine learning project, from start to finish in Python. Navigating between Python libraries is relatively efficient and provides smooth transitions throughout the full workflow of a project, which makes it easy for scientists with little-to-no software development training, like myself, to be able to focus more on the science. By combining dask and xarray, computations could be conducted on large, multi-dimensional datasets through parallel computing. Keras can be used to build and train convolutional neural networks in just a few lines of code. And finally, the large number of scientific computing libraries, ranging from numpy to pandas, can be leveraged to analyze the skill of trained machine learning models and explain results.

To close, I’d like to leave you with some recommended materials for getting started or refining skills in Python and machine learning. On github, scientists at NCAR have made available recent tutorials for Python and machine learning. The books listed here were also useful to me. However, the most helpful thing for me has been to apply these techniques to real research problems! If you have any questions, please feel free to contact me via email.

![](/posts/AMS2021_5-1.png)
